#Introducción

Esta sección tiene como objetivo que tengas una visión general acerca del tema
central de este libro: *La extracción de conocimiento a partir de grandes
cantidades de datos*. Como tal vez te imagines esto no es una tarea sencilla ya
que involucra distintas áreas del conocimiento científico y un buen número de
tecnologías. No te preocupes, vamos a empezar por revisar los conceptos básicos
procurando que tengas una buena idea de donde y cuando aplicar estas técnicas.
Algunas de las preguntas que podrás responder al final del capítulo son:

* ¿Qué buscamos en los datos?
* ¿Cuál es  proceso para el descubrimiento de conocimiento en bases de datos?
* ¿Qué tecnologías intervienen en este proceso?
* ¿Qué técnicas de minería de datos aprenderás a utilizar?
* ¿Dónde podemos aplicar la minería de datos?
* ¿Que retos y limitaciones existen actualmente en el área?

Cuando visitamos al médico, confiamos en que el haya adquirido un
amplio **conocimiento** sobre medicina, a base de interactuar con su entorno y de su
capacidad de procesar la información que lo rodea. Utilizando este
conocimiento él podrá ser capaz de darnos un diagnóstico acertado. 
De la misma manera, si visitamos a un robot-médico, éste deberá contar
con amplio **conocimiento** sobre médicina, aunque en su caso
sea una inteligencia artificial.  
Según Rusell y Norvig [-@russell2016artificial] el término inteligencia
artificial se aplica cuando una máquina imita las funciones cognitivas que
nosotros los humanos asociamos con otras mentes humanas, como por ejemplo:
**aprender** y **resolver problemas**. Precisamente, una de las primeras
aplicaciones de la inteligencia artificial han sido los [Sistemas Expertos][1],
los cuales pueden emular la toma de decisiones de un humano experto, tal como
un médico. Un exponente importante de este tipo de sistemas fue [Mycin][2] el
cual se basaba en un 
[motor de inferencia](http://personales.unican.es/gutierjm/cursos/expertos/Reglas.pdf) 
sencillo, que manejaba una base de
conocimiento de aproximadamente unas 500 reglas. Estas reglas [representaban el
conocimiento][3] de los médicos. Una desventaja de este tipo de sistemas era
que se requería de mucho esfuerzo y recursos para extraer el conocimiento de
los expertos. Se debía entrevistarlos utilizando técnicas de [elicitación del
conocimiento][4] para después [representar dicho
conocimiento](https://es.wikipedia.org/wiki/Representaci%C3%B3n\_del\_conocimiento)
de alguna manera y almacenarlo en una *base de conocimiento*.
Digamos que se realizaba una **extracción manual del conocimiento** a partir de la
experiencia de los expertos. Cualquier desarrollador que ha tenido que "extraer" 
de un usuario los requerimientos de un sistema, te dirá que esa es una tarea cercana a lo imposible.
No es descabellado preguntarnos entonces: ¿el conocimiento "humano" podrá ser
extraído de otras fuentes?, ¿y si contamos con una base de datos de miles de
diagnósticos realizados anteriormente?, ¿podremos extraer de esta base de datos
un nuevo conocimiento?. Este es el tipo de preguntas son las que se trata de
contestar el área de investigación en inteligencia artificial, pero como
veremos más adelante, en la búsqueda de respuestas se involucran muchas otras
áreas de investigación.

## Descubrimiento de conocimiento en bases de datos

(KDD) Al proceso de extraer conocimiento útil a partir de datos se le denomina
*Descubrimiento de Conocimiento en Bases de Datos* o *KDD* ya que es muy común
utilizar las siglas en inglés de "Knowledge Discovery from Databases". Este
proceso puede hacerse manualmente, expertos en algún dominio pueden consultar y
analizar bases de datos para descubrir patrones que les ayuden a tomar
decisiones. Por ejemplo, en la película [The Big Short](https://www.imdb.com/title/tt1596363/)
podemos ver al
inversionista [Michael Burry](https://es.wikipedia.org/wiki/Michael_Burry) 
analizando grandes cantidades de datos, para
después predecir el eminente [desplome de la burbuja inmobiliaria](https://es.wikipedia.org/wiki/Crisis_de_las_hipotecas_subprime). Como podemos
imaginar, este proceso no es trivial, no es simplemente hacer una consulta en
una base de datos o en un motor de búsqueda. Es necesario encontrar patrones
que involucran diferentes variables y relaciones no lineales entre ellas. 
La extracción de conocimiento en bases de datos, en general se realiza de manera
semi-automática, es un proceso en el cual se aplican distintas técnicas como
aprendizaje automático, reconocimiento de patrones, computación inteligente,
estadística, procesamiento de lenguaje natural, visualización, ingeniería de
software entre otras. La Minería de Datos de hecho es solo uno de los componentes del proceso de KDD.
Veamos en que consiste el proceso, según el esquema de Brachman y Anand:

![El proceso del KDD según Brachman y Anand [-brachman1996process].](../img/Proceso-KDD.png)

1. Como primer paso debemos *identificar el objetivo* del proceso de
   KDD. Por ejemplo, un proveedor de telefonía móvil podría estar
   interesado en identificar a aquellos clientes que ya no renovarán
   su contrato y se irán con la competencia. A esto se le llama la
   tasa de cancelación de clientes (en
   inglés [churn rate][5] o
   attrition rate) y es crucial para estimar el desempeño de la
   empresa.

2. El siguiente paso es *seleccionar y recolectar* aquellos datos
   necesarios para el proceso. Para nuestro ejemplo, podríamos
   requerir el historial de pago de los clientes, datos sobre quejas y
   llamadas que han hecho a soporte, servicios adicionales que se han
   contratado o cancelado, etc. Esta información puede estar
   distribuida en diferentes bases de datos. También se puede incluir
   información que se colecte por medio de sensores o sistemas
   externos, por ejemplo lecturas del GPS, caídas de la conexión o el
   número de aplicaciones que el cliente se ha instalado.

3. Es necesario *preprocesar* los datos para limpiarlos de datos
   erróneos, datos faltantes, cambios de formato, inconsistencias,
   etc. Este pude ser  es un proceso muy complicado y puede exigir bastantes
   recursos de la empresa.

4. Dependiendo de los objetivos se deben *transformar* los datos para
   simplificar su procesamiento. Por ejemplo, un documento de texto es necesario
   transformarlo en vector representativo, para facilitar su análisis.
   Una imagen puede ser transformada en
   una representación simplificada pero conservando sus características
   esenciales. Muchas veces también es necesario eliminar campos que no aportan
   mucho a la tarea de KDD. Siguiendo nuestro ejemplo, después de un análisis
   podríamos darnos cuenta que el número de teléfono en sí no es un dato
   importante para distinguir el comportamiento de los clientes, y que al
   contrario la marca y modelo de sus móviles son muy importantes.

5. Este paso consiste en *seleccionar la tarea de minería de datos* adecuada de
acuerdo con la meta establecida para el proceso de KDD. Por ejemplo,
clasificación, regresión, agrupamiento, etc. Estas tareas las vamos a conocer más adelante.

6. Se hace un *análisis exploratorio*, podemos experimentar con diferentes
algoritmos de minería de datos o de aprendizaje automático. Al seleccionar los
algoritmos debemos considerar los tipos de datos que tenemos. Por ejemplo
algunos algoritmos no son apropiados para datos categóricos (modelo de celular).
También debemos ajustar los parámetros para mejorar el desempeño y comparar el
rendimiento entre los algoritmos.

7. En este paso se realiza la *minería de datos* a partir de el o los
algoritmos seleccionados en el paso anterior.

Este paso nos arroja los *patrones ocultos* que describen a los
datos. Siguiendo nuestro ejemplo, el resultado podría ser un conjunto
de reglas que nos pueden servir para decidir si un cliente cancelará
su subscripción. Una de las regla podría ser:

```
SI el cliente tiene un promedio mayor a 7 días de retraso
  AND su promedio de llamadas del mes es menor que 10
ENTONCES:
  el cliente cancelará el servicio
```

Los patrones son entonces **modelos** que se ajustan a los datos. Los modelos no
siempre están expresados en un lenguaje que los humanos podamos entender. Por
ejemplo el resultado de un algoritmo de agrupamiento podrían ser simplemente
grupos de clientes, que después deberíamos de interpretar.

En el texto utilizaremos el término **modelo** en lugar de patrones, ya que es más
usual actualmente. Algo muy importante es que los modelos deben de ser capaces de
representar a objetos no han visto todavía. Digamos nuevos clientes que se subscriban el mes
siguiente. A esta capacidad se le conoce como **generalizar**.

1. En este paso nos toca *interpretar y evaluar* los patrones que encontramos en el paso anterior. Visualizar los patrones y modelos extraídos. Decidir si se ha encontrado algún conocimiento útil.

2. El último paso es hacer algo con el conocimiento adquirido. Se puede utilizar directamente para tomar decisiones, incorporarlo como parte de un sistema o simplemente reportar los resultados a los interesados.

Podemos ver que el proceso completo de KDD requiere en cada paso técnicas y conocimiento 
específico. A continuación veremos las más importantes. 

## Algunas definiciones y tecnologías complementarias

### Minería de datos

El libro se enfocará principalmente en el componente de Minería de Datos, que
como vimos es un paso importante en el proceso de KDD. Pero también te haz dado
cuenta que es muy importante hacer bien los pasos anteriores. De hecho en la
mayoría de los proyectos los otros pasos son los que requieren de mayor esfuerzo.
La minería de datos podemos definirla como: 
**El proceso de búsqueda de patrones aplicando distintos algoritmos a grandes cantidades de datos**. Algunas veces se
utiliza el término Minería de Datos para referirse a todo el proceso de KDD.

### Aprendizaje automático (*Machine Learning*)

El aprendizaje automático es el área de las ciencias computacionales encargada
de estudiar y desarrollar los algoritmos capaces de aprender y hacer
predicciones a partir de los datos. En el proceso de Minería vamos a
aplicar algoritmos de aprendizaje automático, por lo que debemos conocer muy
bien sus fundamentos y limitaciones. Al área en ocasiones le vamos a llamar **Machine Learning**
ya que es muy común utilizar el nombre en inglés.

### Ciencia de datos

Como hemos visto el proceso de KDD busca extraer conocimiento partir de los
datos. ¿Y si vamos más allá?. La ciencia de datos es similar al KDD pero el
conocimiento que se busca es el conocimiento científico. Es hacer ciencia a
partir de los datos. La ciencia apoyada en datos (en inglés data-driven
science) al igual que el KDD es una campo interdisciplinario que incluye
métodos científicos, procesos, y sistemas para extraer conocimiento o
entendimiento a partir de los datos. Últimamente también se le conoce como
Ciencia de Datos, al conjunto de campos de estudio que se relacionan con el
modelado estadístico, matemático y computacional de sistemas. En este caso
podríamos decir que la Ciencia de Datos aporta los fundamentos teóricos (la
ciencia) para las técnicas relaciondas con KDD, IA, Modelado Estadístico, etc.

### Computación inteligente

La expresión computación inteligente (CI de computational intelligence) se
asocia a la habilidad de un sistema de computo de aprender a realizar una tarea
a partir de datos o la observación. Una distinción importante es que la mayoría de sus métodos
son inspirados en la naturaleza. La CI busca resolver problemas
complejos del mundo-real para los cuales el modelado tradicional o matemático
son insuficientes. Los métodos más representativos son: redes neuronales
artificiales, lógica difusa y computación evolutiva.

### *Big Data*

Big Data se refiere al caso de sistemas que cuentan con cantidades enormes de
datos. En un sistema convencional los datos se almacenan en un servidor central. En el caso de
Big Data los datos son tantos que deben almacenarse en muchos servidores.
También el tipo de dato podría ser muy grande, por ejemplo un registro de un
experimento determinado podría medir un terabyte. En estos casos las técnicas o
algoritmos tradicionales resultan inadecuados. El usar Big Data no implica realizar
Minería de Datos, muchos sistemas solamente requieren Big Data para procesar los datos, por
ejemplo para realizar una consulta o algún calculo. Claro que también hay
procesos de Minería de Datos que se realizan utilizando Big Data, esto requiere
de tecnología y algoritmos especializados.

### *Cloud Computing*

¿Quién no ha escuchado esto de la nube?. El Cloud Computing es el nuevo modelo
de desarrollo de sistemas. Básicamente pagamos por el poder computacional más
servicios adicionales como bases de datos, algoritmos en línea y software
pagando solo lo que ocupemos y nada más. Empresas con mucha experiencia en minería de datos
e inmensos recursos computacionales están brindando el servicio de *On-Demand
Machine Learning*. Google con [ml-engine][6], [Amazon ML][7], IBM con
[Watson][8], [Azure][9] de Microsoft entre muchos otros nos permiten hacer todo
el proceso de KDD en sus servidores siguiendo el modelo de Cloud Computing. En
lugar de nosotros implementar los algoritmos, instalar los lenguajes, procesar
los datos,  mantener todo actualizado, etc. podemos contratar el servicio de
Almacenamiento, Procesamiento y Machine Learning. Nuestra labor se volverá
entonces elegir los algoritmos adecuados, preparar los datos y otras tareas más
bien de diseño.

Vamos a implementar nuestros sistemas pensando siempre en la nube como
plataforma de ejecución. Esto nos permitirá desplegar elementos de nuestro
sistema en la nube o utilizar diferentes servicios bajo demanda. Por último, tal
vez después de que diseñes un algoritmo que funcione muy bien, podrías ponerlo a
funcionar en la nube para que lo utilicemos tus clientes.  

## Aplicaciones

Cuando existe el boom de una tecnología, es común que todo lo queramos
resolver con ella. Recientemente tenemos a [Blockchain](https://en.wikipedia.org/wiki/Blockchain) 
una tecnología utilizadas por las criptomonedas, pero que vemos propuestas para utilizarla 
en todos lados, aun cuando no haya una necesidad real.
Esto sucede también con la minería de datos, inteligencia artificial y machine learning.
En ocasiones un simple programa hecho manualmente, puede contener perfectamente el "conocimento" de  
una base de datos. Por esta razón, veamos algunos ejemplos de problemas en los que
si se requiere:

### Modelado de sistemas no lineales

En general se utiliza la Minería de Datos para modelar sistemas no lineales. En
caso de que no estés familiarizado veamos un ejemplo. Ana entra a trabajar a las
7:00 am. El trabajo de Ana está a 10 km de su casa. Ella normalmente sale de su
casa a las 6:20 y llega a las 6:40, hace 20 minutos. En ocasiones sale a las
6:10 y  claro llega a las 6:30. Hasta aquí todo va bien, los tiempos siguen un
comportamiento lineal y constante. Incluso podemos calcular que si antes deja a
sus hijos en la escuela que está a 5 km, llegará en 10 minutos. El problema
viene cuando Ana sale de casa a las 6:25 en ese caso hace 25 minutos y si sale a
las 6:30 hace 40 minutos y llega tarde a su trabajo. ¿Por que el sistema no
sigue un comportamiento lineal?. El problema es que el tiempo que hace a su
trabajo no solo depende de la velocidad promedio y la distancia. El tiempo
depende del tráfico, la ruta que tome, si se pone un policía a dirigir el
tráfico, si hay alguna manifestación, si las primarias están de vacaciones, etc.
El problema se vuelve no lineal por que está ubicado en el mundo real, y si
queremos hacer un calculo más exacto tendríamos que considerar muchas variables,
incluso podríamos hacer un simulador. Cuando salimos a nuestro trabajo muchos
hacemos un cálculo mental del tiempo que haremos por eso los optimistas llegamos
tarde. Pero, ¿en que basamos nuestro cálculo si el sistema es no lineal y por lo
tanto muy complejo?. Así es, en nuestra experiencia, es decir en extraer de
nuestra mente el tiempo que hemos hecho los días anteriores, de nuestro sentido
común "hoy es viernes, habrá mucho tráfico". La minería de datos puede ayudarnos
a modelar este tipo de sistemas, sin que tengamos nosotros que establecer las
relaciones no lineales entre muchas variables.

### Procesamiento del Ruido

El ruido es una complicación común en sistemas del mundo-real. Por ejemplo,
cuando se le pone un sensor a un paciente, en ocasiones las lecturas se ven
afectadas por los movimientos del paciente y registran valores erróneos o muy
altos o bajos en comparación con el valor real. Descubrir patrones en presencia
de ruido es bastante difícil, por lo que se deben elegir técnicas de modelado
que toleren el ruido.  

### Internet de las Cosas

El concepto de Internet de las Cosas (IoT) se refiere básicamente a que todas
las cosas están conectadas al Internet y pueden recopilar e intercambiar datos.
Crear modelos de sistemas no lineales con muchísimas variables es muy
complicado. La tecnología de IoT se puede aplicar en sistemas de ciudades
inteligentes, para agilizar el tráfico en tiempo real o abrir el paso a los
bomberos cuando se dirigen a atender una emergencia. Estos sistemas deben
considerar las lecturas en tiempo real de muchísimos sensores y determinar como
se afectan entre sí, para determinar si hay alguna emergencia grave, predecir el
tráfico. Utilizando técnicas de aprendizaje automático es posible modelar este
tipo de sistemas.

### Reconocimiento de patrones

Al momento de subir a Facebook la tradicional *selfie* con los amigos, el
sistema nos sugiere etiquetar las caras de ellos con su nombre de usuario
correcto. Es decir, el sistema reconoció al usuario a partir de la imagen una
imagen de rostro. Esto es parecido a lo que hicimos anteriormente con las flores
Iris. Pero en lugar de 4 variables de entrada tenemos un arreglo de los cientos
de pixeles que hacen la imagen y en lugar de tres flores, tenemos cientos de
amigos. En el caso de las flores recordamos que ya estaban etiquetadas. En el
caso de Facebook, la extracción del modelo requiere que ya haya ejemplos de los
rostros asociados a los usuarios. Nosotros mismos agregamos ejemplos
clasificados cada que manualmente etiquetamos un rostro. Las mismas técnicas
pueden utilizarse para reconocer gestos, la voz, huellas digitales, firmas,
letra escrita, copias de videos y muchos otros patrones.  

### Negocios

Se utiliza el KDD principalmente para el mercadeo dirigido, para calcular el
riesgo de alguna operación, predecir cambios en los mercados, identificar
tendencias a partir de las publicaciones en redes sociales, recomendar productos
o personalizar servicios.

### Minería de textos

El texto sigue siendo el medio de comunicación más efectivo en Internet. Va desde mensajes cortos tipo SMS o de mansajería instantánea, blogs, correos electrónicos, reportes, artículos, páginas web. La minería de textos sigue siendo uno de los campos con mayor actividad y constantemente se desarrollan nuevas técnicas para su minado. Existen temas que aunque hay avances considerable aun siguen abiertos, como la traducción automática, la generación automática de informes sobre hilos de discusión, clasificación de correos,noticias y otros mensajes. Análisis de Sentimientos de textos, por mencionar algunos. 

## Las Tareas de la Minería de Datos

### Clasificación

Un problema muy común es el de asignar un objeto a una clase o categoría. Veamos
varios ejemplos. Los correos electrónicos que recibimos podemos clasificarlos en
dos clases: *correo válido* o *correo no deseado*. Actualmente en Google Mail
hay otras categorías: *social*, *promociones*, *foros*. La noticia de un
periódico: *deportes*, *nacional*, *internacional*. Un usuario visitando una
tienda en-línea: *compra*, *no-compra*. Una imagen de dígitos escritos a mano:
*0*,*1*,*2*,*3*,*4*,*5*,*6*,*7*,*8*,*9*. La imagen de una [galaxia][10]:
*suave*,*disco*,*estrella*. La imagen de una persona: *ana*, *tom*, *joe*,
*sue*. El problema de Clasificación es el de identificar a que clase
pertenece un objeto, basándose en un conjunto de objetos en los cuales la clase
o categoría ya se conoce. Por ejemplo para el caso del la clasificación de
correos electrónicos, primero debemos contar con un buen número de correos que
han sido clasificados previamente por humanos. A partir de estos datos un
algoritmo de clasificación es capaz de aprender un modelo que podemos utilizar
después para clasificar nuevos correos que nos lleguen. La tarea entonces es
predecir una clase o etiqueta a partir de un conjunto de características.

### Regresión

La Regresión es la misma tarea de la clasificación pero en lugar de asignarle
una clase a cada instancia le asignamos un valor continuo. Por ejemplo, las
instancias podrían ser casas y la variable objetivo podría ser el costo de
venta en el mercado. De nuevo para hacer la predicción nos basamos en un
conjunto de casas que ademas de tener valores en todas las características
cuentan también con el precio de venta.

#### El objetivo del Aprendizaje Supervisado

El objetivo de la Clasificación y la Regresión es el de minimizar el error de
predicción. El error es la diferencia que hay entre el valor o categoría reales
y la predicción. Este es un tema muy importante que veremos en la sección de
[Evaluación de Modelos]. Las dos tareas entonces son ejemplos de aprendizaje
supervisado ya que requieren de datos de entrenamiento para extraer de ahí el
modelo o la predicción directamente.

### Agrupamiento

El agrupamiento es un ejemplo de Aprendizaje no supervisado en el cual no se
requiere de contar con datos previamente clasificados. El objetivo del análisis
de Agrupamiento o *Clustering* es el de encontrar grupos de objetos los cuales
estén estrechamente relacionados o sean similares. El objetivo en este caso es
tratar de que la distancia promedio entre los miembros del mismo grupo sea menor
que la distancia promedio entre objetos de distintos grupos. Por ejemplo, un
sistema de [recuperación de información][11] puede regresar los documentos de
una búsqueda con mayor eficiencia si los documentos similares se guardan juntos.
También podemos encontrar a clientes similares a los cuales les podríamos
ofrecer ciertos servicios.  

### Análisis de asociación

En ocasiones podemos descubrir patrones interesantes los cuales asocian hechos
que con frecuencia suceden al mismo tiempo o en cierto orden. En México por
ejemplo podríamos encontrar que si un consumidor compra cebollas y tomate al
mismo tiempo, es muy probable que también compre carne. Es muy probable que las
tortillas las compre siempre. Normalmente los patrones de asociación se expresan
como reglas de asociación. Para el ejemplo anterior la reglas sería:

{cebollas, tomate} -\> {carne}  

Este tipo de análisis se utiliza para hacer toma de decisiones de marketing,
ubicación de artículos en aparadores, ofrecer agregar algún producto al carrito
de compra, entre otros.

### Detección de anomalías

El objetivo de esta tarea es encontrar objetos que sean muy diferentes al resto.
A este tipo de objetos se les conoce como anomalías o outliers. El reto es no
etiquetar como anomalías a objetos que no lo son y viceversa. Este tipo de
análisis nos permite detectar fraudes con las tarjetas de crédito, robo de
identidad, ciertas enfermedades, problemas en ecosistemas entre otras
aplicaciones.  

### Optimización

La optimización no es en si misma una tarea de la minería de datos, pero la
mayoría de las técnicas que se utilizan para realizar las tareas anteriormente
utilizan algoritmos de optimización. Un algoritmo de optimización podemos verlo
como un algoritmo de búsqueda. Tratamos de encontrar una combinación de objetos
que nos resuelva un problema de manera óptima. En este libro nos enfocaremos en
algoritmos de optimización inspirados en la naturaleza.

#### ¿Buscar o inferir patrones?

Hay casos en los que un algoritmo de optimización puede utilizarse para hacer
minería de datos. El problema de extracción de conocimiento podemos verlo como
la  búsqueda del modelo óptimo de para los datos en cuestión. Por ejemplo
debemos buscar el conjunto de reglas que mejor clasifique ciertos correos
electrónicos. Es importante recordar que la extracción de conocimiento no
siempre se basa en inferencia estadística.

## Los retos que motivan a la Minería de Datos

Tan, Steinbach y Kumar mencionan los principales retos que han motivado el
desarrollo de la minería de datos:

* **Escalabilidad.** Actualmente contamos con inmensas cantidades de datos.
No es raro utilizar
conjuntos de datos que rondan en los terabytes o incluso petabytes. Los
algoritmos, estructuras y bases de datos deben ser capaces de escalar a los
niveles del *Big Data*. Esto sigue siendo un reto, que involucra otras áreas
como el computo distribuido y paralelo, cómputo en la nube. Además se deben de
utilizar técnicas de muestreo.

* **La maldición de la dimensión.** Hace algunos años era inconcebible tener objetos con
miles de características.
Ahora este tipo de instancias son comunes. Si un objeto tuviera solo dos o tres
características lo podríamos representar en 2D o en 3D un eje por cada una. Pero
cuando un objeto tiene muchas características y sobrepasa por mucho el 3D
decimos que se encuentra en un espacio de alta dimensión. Podríamos imaginarnos
que el espacio donde se encuentra ubicado el objeto es cada vez mayor por cada
nueva dimensión que agreguemos y comparativamente el número de objetos que
cabrían en ese espacio sería muchísimo mayor. A este fenómeno se le conoce como
la [maldición de la dimensión][12] pues el volumen del espacio aumenta
exponencialmente haciendo que los datos disponibles se vuelvan dispersos. El
reto es que algunos algoritmos de análisis de datos no dan buenos resultados al
trabajar en altas dimensiones.

![El tamaño del espacio aumenta exponencialmente haciendo que los datos disponibles se vuelvan dispersos](../img/dim.png){ width=70% }

* **Datos Heterogéneos y Complejos.** Los datos que tradicionalmente se han
empleado en análisis de datos son los
categóricos o continuos. En la actualidad contamos con datos muy diversos como
documentos, páginas web, videos, lecturas de sensores, datos con
georreferenciación, series de tiempo, grafos, secuencias DNA entre muchos otros.
Los algoritmos deben considerar las relaciones y propiedades intrínsecas a los
datos, relaciones temporales, propiedades de los grafos, etc.

## Lectura adicional
[Visión General de KDD][13]

[KD Nuggets][14] Es un sitio con mucha información relacionada con minería de datos y KDD.  

[1]:	https://en.wikipedia.org/wiki/Expert_system
[2]:	https://es.wikipedia.org/wiki/Mycin
[3]:	https://es.wikipedia.org/wiki/Representaci%C3%B3n_del_conocimiento
[4]:	http://web.cs.wpi.edu/~jburge/thesis/kematrix.html
[5]:	https://en.wikipedia.org/wiki/Churn_rate
[6]:	https://cloud.google.com/ml-engine/
[7]:	https://aws.amazon.com/machine-learning/
[8]:	https://www.ibm.com/watson/
[9]:	https://azure.microsoft.com/en-us/services/machine-learning/
[10]:	https://www.galaxyzoo.org/
[11]:	https://es.wikipedia.org/wiki/B%C3%BAsqueda_y_recuperaci%C3%B3n_de_informaci%C3%B3n
[12]:	https://es.wikipedia.org/wiki/Maldici%C3%B3n_de_la_dimensi%C3%B3n
[13]:	http://www.kdnuggets.com/gpspubs/aimag-kdd-overview-1996-Fayyad.pdf
[14]:	http://www.kdnuggets.com/

[medico]:	../img/doctor.jpg
[image-2]:	../img/Proceso-KDD.png
[image-3]:	../img/miner.jpg
[image-4]:	../img/trafico.jpg
[image-5]:	../img/superheroes.jpg
[image-6]:	../img/dim.png
